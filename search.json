[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ReinforcedLearningClass",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\\[\nx^2 + 10 = 100\n\\tag{1}\\]\nPrueba Ecuación 1\nReferencia ejemplo Ayers (2005) Referencia ejemplo (see Ayers 2005, 52-53; B. Thomas Jr 2010, cap. 1)\nBlack-Scholes (Ecuación 2) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C\n\\tag{2}\\]\n\n\n\n\nAyers, G. 2005. «Air Pollution and Climate Change: Has Air Pollution Suppressed Rainfall over Australia?» 39 (2): 51-57. https://search.informit.org/doi/10.3316/informit.632702153657460.\n\n\nB. Thomas Jr, George. 2010. Cálculo varias variables. Doceava. Addison-Wesley."
  },
  {
    "objectID": "Tarea1.html",
    "href": "Tarea1.html",
    "title": "1  Tarea 1 (Fecha de Entrega 20 Septiembre 2024 12:00:00)",
    "section": "",
    "text": "Exercise 1.1 Read (Sec 1.1, pp 1-2 Sutton and Barto 2018) and answer the following. Explain why Reinforcement Learning differs for supervised and unsupervised.\n\n\nEl aprendizaje supervisado requiere de ejemplos de soluciones. Mientras que el reforzado requiere una función de valor.\n\n\nExercise 1.2 See the first Brunton’s youtube about Reinforced Learning. Then acordingly to its presentation explain what is the meaning of the following expression.\n\\[\nV_\\pi(s) = E\\left(\\sum_t\\gamma^tr_t\\mid s_0=s\\right)\n\\]\n\n\nLa expresión presentada en el video Reinforcement Learning. \\[\nV_\\pi (s) = E\\left[\\left. \\sum_{t}\\gamma^t r_t \\right| s_0 = s\\right]\n\\]\nhace referencia a la función de valor del problema de optimización representada por la recompensa esperada dado la politica \\(\\pi\\) y el estado inicial \\(s\\). Aquí \\(\\gamma\\) es el factor de descuento y \\(r_t\\) es la recompensa por etapa \\(t\\).\n\n\nExercise 1.3 Form (see Sutton and Barto 2018, sec. 1.7) obtain a time line pear year from 1950 to 2012.\n\n\nlibrary(devtools)\n\nWarning: package 'devtools' was built under R version 4.3.3\n\n\nLoading required package: usethis\n\n\nWarning: package 'usethis' was built under R version 4.3.3\n\nlibrary(milestones)\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.3\n\n#library(bibtex)\n## Activate the Core Packages\n#biblio &lt;- bibtex::read.bib(\"references.bib\")\n\n## Initialize defaults\ncolumn &lt;- lolli_styles()\n\ndata &lt;- read_csv(col_names=TRUE, show_col_types=FALSE, file='rl_time_line.csv')\n\n\ndata &lt;- data |&gt; arrange(date)\n\n## Build a table\ngt(data) |&gt;\n  #cols_hide(columns = event) |&gt;\n  tab_style(cell_text(v_align = \"top\"),\n            locations = cells_body(columns = date)) |&gt;\n  tab_source_note(source_note = \"Source: Sutton and Barto (2018)\") \n\n\n\n\n\n  \n    \n      date\n      event\n      reference\n    \n  \n  \n    1911\nThe Law Effect\nThorndike, 1911\n    1950\n\"optimal control\" term\nMR0090477 Bellman, Richard Dynamic programming. Princeton University Press, Princeton, NJ, 1957. xxv+342 pp.\n    1954\nTrial and Error Learning\nMinsky,Farley, Clark 1954\n    1957\nDynamic Programming\nMR0090477 Bellman, Richard Dynamic programming. Princeton University Press, Princeton, NJ, 1957. xxv+342 pp.\n    1957\nMarkovian Decision Processes\nMR0091859 Bellman, Richard A Markovian decision process.J. Math. Mech.6(1957), 679-684.\n    1960\nPolicy iteration method\nRon Howard (1960)\n    1960\n\"Reinforcement\" and \"R. Learning\" terms\nNA\n    1960\nLearning with a critic\nHoff, 1960\n    1961\nSteps Toward Artificial Inteligence\nMinsky, 1961\n    1963\nTic Tac Toe\nDonald Michie, 1961\n    1968\nGLEE and BOXES\nMichie and Chambers 1968\n    1982\nAsynchronous methods\nBertsekas, 1982\n    1982\nModern Dynamic Programming\nWhite 1982\n    1983\nAsynchronous methods\nMR0712113 Bertsekas, Dimitri P. Distributed asynchronous computation of fixed points.Math. Programming27(1983), no.1, 107-120.\n    1983\nModern Dynamic Programming\nMR0749232 Ross, Sheldon Introduction to stochastic dynamic programming. Probab. Math. Statist. Academic Press, Inc. [Harcourt Brace Jovanovich, Publishers], New York, 1983. xi+164 pp. ISBN:0-12-598420-0\n    1983\nModern Dynamic Programming\nWhite 1983\n    1985\nMany Aplications\nMR1295629 White, D. J. Markov decision processes.John Wiley & Sons, Ltd., Chichester, 1993. xiv+224 pp. ISBN:0-471-93627-8\n    1988\nMany Aplications\nMR1295629 White, D. J. Markov decision processes.John Wiley & Sons, Ltd., Chichester, 1993. xiv+224 pp. ISBN:0-471-93627-8\n    1991\nPartially Observable MDPs\nMR1105166 Lovejoy, William S.A survey of algorithmic methods for partially observed Markov decision processes.Ann. Oper. Res.28(1991), no.1-4, 47-65\n    1993\nMany Aplications\nMR1200993 White, D. J.Markov decision processes: discounted expected reward or average expected reward?.J. Math. Anal. Appl.172(1993), no.2, 375-384.\n    1994\nModern Dynamic Programming\nPuterman, 1994\n    1995\nModern Dynamic Programming\nBertsekas, 1995\n    1996\nAproximathon methods\nMR1416619 Rust, John Numerical dynamic programming in economics.Handbook of computational economics, Vol. I, 619-729. Handbooks in Econom., 13 North-Holland Publishing Co., Amsterdam, 1996 ISBN:0-444-89857-3\n  \n  \n    \n      Source: Sutton and Barto (2018)\n    \n  \n  \n\n\n\ncolumn$color &lt;- \"orange\"\ncolumn$size  &lt;- 15\ncolumn$source_info &lt;- \"Source: Sutton and Barto (2018)\"\n\n## Milestones timeline\nmilestones(datatable = data, styles = column)\n\n\n\n\n\nExercise 1.4 Consider the following comsuption-saving problem with dynamics \\[\nx_{k+1} = (1+r)(x_k-a_k), k = 0,1,\\ldots,N-1\n\\]\nand utility function\n\\[\n\\beta^N(x_N)^{1-\\gamma} + \\sum_{k=0}^{N-1}\\beta^k(a_k)^{1-\\gamma}.\n\\]\nShow that the value functions of the DP alghorithm the form\n\\[\nJ_k(x) = A_kk\\beta^k x^{1-\\gamma},\n\\]\nwhere \\(A_N=1\\) and for \\(k=N-1,\\ldots,0\\),\n\\[\nA_k = \\left[1 + \\left((1+r)\\beta A_{k+1}\\right)^{1/\\gamma}\\right]^\\gamma.\n\\]\nShow also that the optimal policies are \\(h_k(x) = A^{-1/\\gamma}x\\), for \\(k = N-1,\\ldots,0\\).\n\n\n\nConsiderando \\(J_{N}\\) como sigue\n\\[\nJ_{N}\\left(x\\right)=\\beta^{N}x^{1-\\gamma}K_{N},\n\\]\ncon \\(K_{N}=1\\) bajo la hipótesis de que \\[\nc_{k}\\left(x,a\\right)=\\beta^{k}a^{1-\\gamma}\n\\] calculamos \\(J_{N-1}\\).\n\\[\\begin{align*}\nJ_{N-1}\\left(x\\right) & =\\max_{a\\in A\\left(x\\right)}\\left\\{ c_{N-1}(x,a)+J_{N}\\left((1+i)(x-a)\\right)\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ \\beta^{N-1}a^{1-\\gamma}+\\beta^{N}\\left((1+i)(x-a)\\right)^{1-\\gamma}\\right\\}\n\\end{align*}\\]\nDefinimos el argumento como una función \\(q\\).\n\\[\\begin{align*}\nq(x,a) & =\\beta^{N-1}a^{1-\\gamma}+\\beta^{N}\\left((1+i)(x-a)\\right)^{1-\\gamma}\\\\\n& =C_{1}a^{1-\\gamma}+C_{2}\\left(x-a\\right)^{1-\\gamma},\n\\end{align*}\\]\ndonde \\(C_{1}=\\beta^{N-1}\\) y \\(C_{2}=\\beta^{N}(1+i)^{1-\\gamma}K_{N}.\\) Como \\(q\\) es continua en \\(\\left(x,a\\right)\\). Podemos calcular el máximo mediante el gradiente.\n\\[\n\\partial_{a}q=C_{1}\\left(1-\\gamma\\right)a^{-\\gamma}-C_{2}(1-\\gamma)\\left(x-a\\right)^{-\\gamma}.\n\\]\nIgualando, \\(\\partial_{a}q=0\\). \\[\\begin{align*}\nC_{1}a^{-\\gamma} & =C_{2}\\left(x-a\\right)^{-\\gamma}\\\\\n\\dfrac{C_{1}}{C_{2}} & =\\left(\\dfrac{x-a}{a}\\right)^{-\\gamma}\\\\\n\\left(\\dfrac{C_{1}}{C_{2}}\\right)^{-\\frac{1}{\\gamma}} & =\\frac{x}{a}-1\\\\\n\\left(\\dfrac{C_{1}}{C_{2}}\\right)^{-\\frac{1}{\\gamma}}+1 & =\\frac{x}{a}\\\\\na & =\\dfrac{x}{\\left(\\dfrac{C_{1}}{C_{2}}\\right)^{-\\frac{1}{\\gamma}}+1}\n\\end{align*}\\]\nFinalmente \\[\na=h(x)=\\dfrac{x}{\\left(\\beta(1+i)^{1-\\gamma}\\right)^{\\frac{1}{\\gamma}}+1}\n\\]\nDefiniendo \\(\\eta=\\left(\\beta(1+i)^{1-\\gamma}\\right)^{\\frac{1}{\\gamma}}+1,\\) \\(\\eta-1=\\left(\\beta(1+i)^{1-\\gamma}\\right)^{\\frac{1}{\\gamma}}\\)\nentonces \\[\nh(x)=\\dfrac{x}{\\eta},\n\\]\n\\[\\begin{align*}\nJ_{N-1}(x) & =\\beta^{N-1}\\left(\\dfrac{x}{\\eta}\\right)^{1-\\gamma}+\\beta^{N}\\left((1+i)\\left(x-\\dfrac{x}{\\eta}\\right)\\right)^{1-\\gamma}\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\left(\\eta^{\\gamma-1}+\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\dfrac{\\eta-1}{\\eta}\\right)^{1-\\gamma}\\right)\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\eta^{\\gamma-1}\\left(1+\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\eta-1\\right)^{1-\\gamma}\\right)\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\eta^{\\gamma-1}\\left(1+\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\eta-1\\right)^{1-\\gamma}\\right)\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\eta^{\\gamma-1}\\left(1+\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\left(\\beta(1+i)^{1-\\gamma}\\right)^{\\frac{1}{\\gamma}}\\right)^{1-\\gamma}\\right)\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\eta^{\\gamma-1}\\left(1+\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\beta(1+i)^{1-\\gamma}\\right)^{\\frac{1}{\\gamma}-1}\\right)\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\eta^{\\gamma-1}\\left(1+\\beta^{\\frac{1}{\\gamma}}(1+i)^{\\left(1-\\gamma\\right)\\left(\\frac{1}{\\gamma}-1\\right)+1-\\gamma}\\right)\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\eta^{\\gamma-1}\\left(1+\\beta^{\\frac{1}{\\gamma}}(1+i)^{\\left(\\frac{1}{\\gamma}-1\\right)}\\right)\\\\\n& =\\beta^{N-1}x^{1-\\gamma}\\eta^{\\gamma},\n\\end{align*}\\]\nEntonces \\[\nK_{N-1}=\\eta^{\\gamma},h_{k-1}\\left(x\\right)=\\dfrac{x}{\\left(K_{N-1}\\right)^{1/\\gamma}}\n\\]\nAhora calculamos \\(J_{N-2}\\)\n\\[\\begin{align*}\nJ_{N-2}\\left(x\\right) & =\\max_{a\\in A\\left(x\\right)}\\left\\{ \\beta^{N-2}a^{1-\\gamma}+\\beta^{N-1}\\left[\\left(1+i\\right)\\left(x-a\\right)\\right]^{1-\\gamma}\\eta^{\\gamma}\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ q\\left(x,a\\right)\\right\\} ,\n\\end{align*}\\]\ndonde \\[\nq\\left(x,a\\right)=C_{1}a^{1-\\gamma}+C_{2}\\left(x-a\\right)^{1-\\gamma},\n\\]\ncon \\(C_{1}=\\beta^{N-2}\\) y \\(C_{2}=\\beta^{N-1}\\left(1+i\\right)^{1-\\gamma}K_{N-1}\\) . Obteniendo, por recursividad \\[\\begin{align*}\nh_{N-2} & =\\dfrac{x}{\\left(\\dfrac{C_{1}}{C_{2}}\\right)^{-\\frac{1}{\\gamma}}+1}\\\\\n& =\\dfrac{x}{\\left(\\dfrac{1}{\\beta\\left(1+i\\right)^{1-\\gamma}K_{N-1}}\\right)^{-\\frac{1}{\\gamma}}+1}\\\\\n& =\\dfrac{x}{\\left(\\beta\\left(1+i\\right)^{1-\\gamma}K_{N-1}\\right)^{\\frac{1}{\\gamma}}+1}\n\\end{align*}\\]\nEntonces, sea \\[\\begin{align*}\n\\eta' & =\\left(\\beta\\left(1+i\\right)^{1-\\gamma}K_{N-1}\\right)^{\\frac{1}{\\gamma}}+1.\n\\end{align*}\\]\nRepitiendo, el caso anterior, tenemos que \\[\\begin{align*}\nJ_{N-2}\\left(x\\right) & =\\beta^{N-2}x^{1-\\gamma}\\eta_{'}^{\\gamma-1}\\left(1+K_{N-1}\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\left(\\beta(1+i)^{1-\\gamma}K_{N-1}\\right)^{\\frac{1}{\\gamma}}\\right)^{1-\\gamma}\\right)\\\\\n& =\\beta^{N-2}x^{1-\\gamma}\\eta_{'}^{\\gamma-1}\\left(1+K_{N-1}\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\left(\\beta(1+i)^{1-\\gamma}K_{N-1}\\right)^{\\frac{1}{\\gamma}}\\right)^{1-\\gamma}\\right)\\\\\n& =\\beta^{N-2}x^{1-\\gamma}\\eta_{'}^{\\gamma-1}\\left(1+K_{N-1}\\beta\\left(1+i\\right)^{1-\\gamma}\\left(\\beta(1+i)^{1-\\gamma}K_{N-1}\\right)^{\\frac{1}{\\gamma}-1}\\right)\\\\\n& =\\beta^{N-2}x^{1-\\gamma}\\eta_{'}^{\\gamma-1}\\left(1+K_{N-1}\\beta\\left(1+i\\right)^{1-\\gamma}(1+i)^{\\left(1-\\gamma\\right)\\left(\\frac{1}{\\gamma}-1\\right)}K_{N-1}^{\\frac{1}{\\gamma}-1}\\right)\\\\\n& =\\beta^{N-2}x^{1-\\gamma}\\eta_{'}^{\\gamma-1}\\left(1+K_{N-1}\\beta^{1/\\gamma}\\left(1+i\\right)^{\\frac{1}{\\gamma}-1}K_{N-1}^{\\frac{1}{\\gamma}-1}\\right)\\\\\n& =\\beta^{N-2}x^{1-\\gamma}\\eta_{'}^{\\gamma-1}\\left(1+\\beta^{1/\\gamma}\\left(1+i\\right)^{\\frac{1}{\\gamma}-1}K_{N-1}^{\\frac{1}{\\gamma}}\\right)\\\\\n& =\\beta^{N-2}x^{1-\\gamma}\\eta'{}^{\\gamma},\n\\end{align*}\\]\nentonces \\[\nK_{N-2}=\\eta'{}^{\\gamma},\n\\]\ny \\[\nh_{N-2}=\\dfrac{x}{K_{N-2}^{1/\\gamma}}\n\\]\nPor lo tanto, tenemos que \\[\nK_{n}=\\left(\\beta\\left(1+i\\right)^{1-\\gamma}K_{n+1}\\right)^{\\frac{1}{\\gamma}}+1,n=0,1,2,\\ldots,N,\n\\] con \\(K_{N}=1\\).\nObteniendo así \\[\\begin{align*}\nJ_{n}\\left(x\\right) & =\\beta^{n}x^{1-\\gamma}K_{n}\\\\\nh_{n}\\left(x\\right) & =\\dfrac{x}{K_{n}^{1/\\gamma}}\n\\end{align*}\\]\n\n\n\nExercise 1.5 Consider now the infinite-horizon version of the above compsumption problem.\n\nWrite down the associated Bellman equation.\nArgue why a solution to the Bellman equation should be the form\n\n\\[\nv(x) = cx^{1-\\gamma},\n\\]\nwhere \\(c\\) is constant. Find the constant \\(c\\) and the stationary optimal policy.\n\n\nPara el caso infinito. Estamos considerando \\[\nc\\left(x,a\\right)=a^{1-\\gamma}\n\\]\nEntonces \\[\n\\nu\\left(x\\right)=\\max_{a\\in A\\left(x\\right)}\\left\\{ a^{1-\\gamma}+\\beta\\nu\\left(\\left(1+i\\right)\\left(x-a\\right)\\right)\\right\\} ,\n\\]\nconsiderando \\(\\nu\\left(x\\right)=cx^{1-\\gamma}.\\) Entonces \\[\n\\nu\\left(x\\right)=\\max_{a\\in A\\left(x\\right)}\\left\\{ a^{1-\\gamma}+\\beta c\\left[\\left(1+i\\right)\\left(x-a\\right)\\right]^{1-\\gamma}\\right\\} ,\n\\]\ndefinimos \\[\nq\\left(x,a\\right)=a^{1-\\gamma}+\\beta c\\left[\\left(1+i\\right)\\left(x-a\\right)\\right]^{1-\\gamma},\n\\] entonces \\[\n\\partial_{a}q=\\left(1-\\gamma\\right)a^{-\\gamma}+\\beta c\\left(1-\\gamma\\right)\\left(1+i\\right)^{1-\\gamma}\\left(-1\\right)\\left(x-a\\right)^{-\\gamma}.\n\\]\nSi \\(\\partial_{a}q=0\\). Entonces\n\\[\\begin{align*}\na^{-\\gamma} & =\\beta c\\left(1+i\\right)^{1-\\gamma}\\left(x-a\\right)^{-\\gamma}\\\\\n\\left(\\beta c\\left(1+i\\right)^{1-\\gamma}\\right)^{-1} & =\\left(\\dfrac{x-a}{a}\\right)^{-\\gamma}\\\\\n\\beta^{-1}c^{-1}\\left(1+i\\right)^{\\gamma-1} & =\\left(\\dfrac{x}{a}-1\\right)^{-\\gamma}\\\\\n\\left[\\beta^{-1}c^{-1}\\left(1+i\\right)^{\\gamma-1}\\right]^{-\\frac{1}{\\gamma}}+1 & =\\dfrac{x}{a}\n\\end{align*}\\]\nPor lo tanto \\[\\begin{align*}\na & =\\dfrac{x}{\\left[\\beta^{-1}c^{-1}\\left(1+i\\right)^{\\gamma-1}\\right]^{-\\frac{1}{\\gamma}}+1}\\\\\n& =\\dfrac{x}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\n\\end{align*}\\]\nAhora remplazamos en \\(q\\) \\[\\begin{align*}\n\\nu\\left(x\\right) & =\\left(\\dfrac{x}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}+\\beta c\\left[\\left(1+i\\right)\\left(x-\\dfrac{x}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)\\right]^{1-\\gamma}\\\\\n& =x^{1-\\gamma}\\left(\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)+x^{1-\\gamma}\\left(1+i\\right)^{1-\\gamma}\\beta c\\left(1-\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}\\\\\n& =x^{1-\\gamma}\\left[\\left(\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)+\\left(1+i\\right)^{1-\\gamma}\\beta c\\left(1-\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}\\right].\n\\end{align*}\\] Entonces\n\\[\\begin{align*}\nc & =\\left(\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}+\\left(1+i\\right)^{1-\\gamma}\\beta c\\left(1-\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}\\\\\n& =\\left(\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}+\\left(1+i\\right)^{1-\\gamma}\\beta c\\left(\\dfrac{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}\\\\\n& =\\left(\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}\\left(1+\\left(1+i\\right)^{1-\\gamma}\\beta c\\left(\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}\\right)^{1-\\gamma}\\right)\\\\\n& =\\left(\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}\\left(1+\\left(1+i\\right)^{1-\\gamma}\\beta c\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}-1}\\right)\\\\\n& =\\left(\\dfrac{1}{\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1}\\right)^{1-\\gamma}\\left(1+\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}\\right)\\\\\n& =\\left(\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1\\right)^{\\gamma-1}\\left(1+\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}\\right)\\\\\nc & =\\left(\\left[\\beta c\\left(1+i\\right)^{1-\\gamma}\\right]^{\\frac{1}{\\gamma}}+1\\right)^{\\gamma}\n\\end{align*}\\]\nAhora, nos queda despejar \\(c\\). \\[\\begin{align*}\nc^{\\frac{1}{\\gamma}} & =\\beta^{\\frac{1}{\\gamma}}c^{\\frac{1}{\\gamma}}\\left(1+i\\right)^{\\frac{1}{\\gamma}-1}+1\\\\\n1 & =\\beta^{\\frac{1}{\\gamma}}\\left(1+i\\right)^{\\frac{1}{\\gamma}-1}+c^{-\\frac{1}{\\gamma}}\\\\\nc^{-\\frac{1}{\\gamma}} & =1-\\beta^{\\frac{1}{\\gamma}}\\left(1+i\\right)^{\\frac{1}{\\gamma}-1}\\\\\nc & =\\left(1-\\beta^{\\frac{1}{\\gamma}}\\left(1+i\\right)^{\\frac{1}{\\gamma}-1}\\right)^{-\\gamma}\n\\end{align*}\\]\n\n\nExercise 1.6 Let \\(\\{\\xi_k\\}\\) be a dynamics of iid random variables such that \\(E\\left[\\xi\\right] = 0\\) and \\(E\\left[\\xi^2\\right] = d\\). Consider the dynamics \\[\nx_{k+1} = x_k + a_k + \\xi_k, k = 0,1,2,\\ldots,\n\\]\nand the discounted cost \\[\nE\\left[ \\sum \\beta^k \\left(a^2_k + x_k^2\\right)\\right]\n\\]\n\nWrite down the associated Bellman equation.\nConjecture that the solution to the Bellman equation takes the form \\(v(x) = ax^2 + b\\), where \\(a\\) and \\(b\\) are constant.\nDetermine the constants \\(a\\) and \\(b\\).\nConjecture that the solution to the Bellman equation takes the form \\(v(x) = ax^2 + b\\), where \\(a\\) and \\(b\\) are constant. Determine the constants \\(a\\) and \\(b\\).\n\n\n\n\\[\\begin{align*}\n\\nu\\left(x\\right) & =\\max_{a\\in A\\left(x\\right)}\\left\\{ c\\left(x,a\\right)+E\\left[\\nu\\left(f\\left(x,a\\right)\\right)\\right]\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ a^{2}+x^{2}+E\\left[\\nu\\left(x+a+\\xi\\right)\\right]\\right\\}\n\\end{align*}\\]\nPara \\(\\nu\\left(x\\right)=ax^{2}+b\\)\n\\[\\begin{align*}\n\\nu\\left(x\\right) & =\\max_{a\\in A\\left(x\\right)}\\left\\{ c\\left(x,a\\right)+\\beta E\\left[\\nu\\left(f\\left(x,a\\right)\\right)\\right]\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ A^{2}+x^{2}+\\beta\\left(E\\left[a\\left(f^{2}\\left(x,a\\right)\\right)\\right]+b\\right)\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ A^{2}+x^{2}+\\beta\\left(aE\\left[f^{2}\\left(x,a\\right)\\right]+b\\right)\\right\\}\n\\end{align*}\\]\nNotemos que \\[\\begin{align*}\nE\\left[f^{2}\\left(x,A\\right)\\right] & =E\\left[\\left(x+A+\\xi\\right)^{2}\\right]\\\\\n& =E\\left[x^{2}+A^{2}+\\xi^{2}+2xA+2x\\xi+2\\xi A\\right]\\\\\n& =x^{2}+A^{2}+E\\left[\\xi^{2}\\right]+2xA+2xE\\left[\\xi\\right]+2AE\\left[\\xi\\right]\\\\\n& =x^{2}+A^{2}+d+2xA\n\\end{align*}\\]\nEntonces \\[\\begin{align*}\nax^{2}+b & =\\max_{a\\in A\\left(x\\right)}\\left\\{ A^{2}+x^{2}+\\beta\\left[a\\left(x^{2}+A^{2}+d+2xA\\right)+b\\right]\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ A^{2}+x^{2}+\\beta a\\left(x^{2}+A^{2}+d+2xA\\right)+\\beta b\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ A^{2}+x^{2}+\\beta ax^{2}+\\beta aA^{2}+\\beta ad+2\\beta axA+\\beta b\\right\\} \\\\\n& =\\max_{a\\in A\\left(x\\right)}\\left\\{ A^{2}\\left(\\beta a+1\\right)+2\\beta axA+x^{2}+\\beta ax^{2}+\\beta ad+\\beta b\\right\\}\n\\end{align*}\\]\nDefinimos \\[\nw\\left(x,A\\right)=A^{2}\\left(\\beta a+1\\right)+2\\beta axA+x^{2}+\\beta ax^{2}+\\beta ad+\\beta b,\n\\]\nentonces \\[\n\\partial_{A}w=2A\\left(\\beta a+1\\right)+2\\beta ax.\n\\]\nSi \\(\\partial_{A}w=0\\), entonces \\[\nA=-\\dfrac{\\beta ax}{\\beta a+1}\n\\]\nEntonces \\[\\begin{align*}\n\\nu\\left(x\\right) & =\\left(\\beta ax\\right)^{2}-2\\dfrac{\\left(\\beta ax\\right)^{2}}{\\beta a+1}+x^{2}+\\beta ax^{2}+\\beta ad+\\beta b\\\\\n& =x^{2}\\left(\\left[\\beta a\\right]^{2}-2\\dfrac{\\left(\\beta a\\right)^{2}}{\\beta a+1}+1+\\beta a\\right)+\\beta ad+\\beta b\n\\end{align*}\\]\nEntonces \\[\\begin{align*}\na & =\\left[\\beta a\\right]^{2}-2\\dfrac{\\left(\\beta a\\right)^{2}}{\\beta a+1}+1+\\beta a\\\\\nb & =\\beta ad+\\beta b,\n\\end{align*}\\] de forma rapida\n\\[\nb=\\dfrac{\\beta ad}{1-\\beta},\n\\] entonces queda pendiente calcular \\(a\\)\n\\[\\begin{align*}\na & =\\left[\\beta a\\right]^{2}-2\\dfrac{\\left(\\beta a\\right)^{2}}{\\beta a+1}+1+\\beta a.\\\\\n0 & =\\left(\\beta a\\right)^{2}\\left(1-\\dfrac{2}{\\beta a+1}\\right)+1+\\left(\\beta-1\\right)a\\\\\n& =\\left(\\beta a\\right)^{2}\\left(\\beta a+1-2\\right)+\\beta a+1+\\left(a\\beta-a\\right)\\left(\\beta a+1\\right)\\\\\n& =\\left(\\beta a\\right)^{2}\\left(\\beta a-1\\right)+\\beta a+1+\\left[\\left(a\\beta\\right)^{2}+a\\beta-\\beta a^{2}-a\\right]\\\\\n& =\\left(\\beta a\\right)^{3}+2a\\beta+1-\\beta a^{2}-a\\\\\n& =\\beta^{3}a^{3}-\\beta a^{2}+\\left(2\\beta-1\\right)a+1\n\\end{align*}\\]\nConcluyendo que la constante \\(b\\) depende de \\(a\\) y \\(a\\) es una solución, dependiente de \\(\\beta\\), de la ecuación cúbica que"
  },
  {
    "objectID": "Extra1.html#verificar-la-ecuación-de-bellman-para-el-problema-de-gridworld-de-la-casilla-s-22",
    "href": "Extra1.html#verificar-la-ecuación-de-bellman-para-el-problema-de-gridworld-de-la-casilla-s-22",
    "title": "3  Ejercicio Extra",
    "section": "3.1 Verificar la ecuación de Bellman para el problema de GridWorld de la casilla \\(s = (2,2)\\)",
    "text": "3.1 Verificar la ecuación de Bellman para el problema de GridWorld de la casilla \\(s = (2,2)\\)\nPor la ecuación de Bellman.\n\\[\\begin{align*}\nv_{\\pi}\\left(s\\right) & =E_{\\pi}\\left[G_{t}\\mid S_{t}=s\\right]\\\\\n& =\\sum_{a}\\pi\\left(a\\mid s\\right)\\sum_{s',r}p\\left(s^{'},r\\mid s,a\\right)\\left[r+\\gamma v_{\\pi}\\left(s'\\right)\\right]\n\\end{align*}\\]\nQueremos calcular \\(v_{\\pi}\\left(s\\right)\\) donde \\(s_{0}=\\left(2,2\\right)\\) Considerando el \\(\\left(0,0\\right)\\) la esquina superior izquierda. Comenzaremos revisando \\(p\\left(s^{'},r\\mid s,a\\right)\\). Notemos que en Gridword solo son posibles las recompensas \\([-1,0,5,10]\\) según la posición actual y la acción \\(a\\). Para nuestro caso, \\(s_{0}\\) \\[\np\\left(s',r\\mid s_{0},a\\right)=0,r\\in[-1,5,10],\\forall a,\\forall s'.\n\\]\nPor lo anterior la ecuación de Bellman queda como sigue \\[\nv_{\\pi}\\left(s\\right)=\\sum_{a}\\pi\\left(a\\mid s\\right)\\sum_{s'}p\\left(s^{'},0\\mid s_{0},a\\right)\\left[\\gamma v_{\\pi}\\left(s'\\right)\\right].\n\\]\nDefinamos la función auxiliar \\[\ng\\left(a\\right)=\\sum_{s'}p\\left(s^{'},0\\mid s_{0},a\\right)\\left[\\gamma v_{\\pi}\\left(s'\\right)\\right].\n\\]\nPara \\(s=s_{0}\\) y \\(a\\) fijo. \\[\ng\\left(a\\right)=\\gamma v_{\\pi}\\left(s'\\right),\n\\]\ndonde \\(s'\\) satisface \\(\\mathcal{P}\\left(s'\\mid s,a\\right)=1\\). Para el ejercicio, \\(v_{\\pi}\\left(s'\\right)\\) estan dadas para \\(s'=\\left(1,2\\right),s'=\\left(2,1\\right),s'=\\left(2,3\\right),s'=\\left(3,2\\right)\\), y estamos suponiendo una distribución uniforme en \\(\\pi\\), entonces\n\\[\n\\pi\\left(a\\mid s\\right)=\\dfrac{1}{4},\\forall s.\n\\]\nFinalmente,\n\\[\\begin{align*}\nv_{\\pi}\\left(s\\right) & =\\dfrac{1}{4}\\left(\\gamma\\left(2.3+0.7+0.4-0.4\\right)\\right)\\\\\n& =\\dfrac{3}{4}\\gamma\\approx0.7\n\\end{align*}\\]"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ayers, G. 2005. “Air Pollution and Climate Change: Has Air\nPollution Suppressed Rainfall over Australia?” 39 (2): 51–57. https://search.informit.org/doi/10.3316/informit.632702153657460.\n\n\nB. Thomas Jr, George. 2010. Cálculo Varias Variables. Doceava.\nAddison-Wesley."
  }
]