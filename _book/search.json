[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ReinforcedLearningClass",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "Tarea1.html",
    "href": "Tarea1.html",
    "title": "1  Tarea 1 (Fecha de Entrega 20 Septiembre 2024 12:00:00)",
    "section": "",
    "text": "Exercise 1.1 Read (Sec 1.1, pp 1-2 Sutton and Barto 2018) and answer the following. Explain why Reinforcement Learning differs for supervised and unsupervised.\n\n\nExercise 1.2 See the first Brunton’s youtube about Reinforced Learning. Then acordingly to its presentation explain what is the meaning of the following expression.\n\\[\nV_\\pi(s) = E\\left(\\sum_t\\gamma^tr_t\\mid s_0=s\\right)\n\\]\n\n\nLa expresión presentada en el video Reinforcement Learning. \\[\nV_\\pi (s) = E\\left[\\left. \\sum_{t}\\gamma^t r_t \\right| s_0 = s\\right]\n\\]\nhace referencia a la función de valor del problema de optimización representada por la recompensa esperada dado la politica \\(\\pi\\) y el estado inicial \\(s\\). Aquí \\(\\gamma\\) es el factor de descuento y \\(r_t\\) es la recompensa por etapa \\(t\\).\n\n\nExercise 1.3 Form (see Sutton and Barto 2018, sec. 1.7) obtain a time line pear year from 1950 to 2012.\n\n\nExercise 1.4 Consider the following comsuption-saving problem with dynamics \\[\nx_{k+1} = (1+r)(x_k-a_k), k = 0,1,\\ldots,N-1\n\\]\nand utility function\n\\[\n\\beta^N(x_N)^{1-\\gamma} + \\sum_{k=0}^{N-1}\\beta^k(a_k)^{1-\\gamma}.\n\\]\nShow that the value functions of the DP alghorithm the form\n\\[\nJ_k(x) = A_kk\\beta^k x^{1-\\gamma},\n\\]\nwhere \\(A_N=1\\) and for \\(k=N-1,\\ldots,0\\),\n\\[\nA_k = \\left[1 + \\left((1+r)\\beta A_{k+1}\\right)^{1/\\gamma}\\right]^\\gamma.\n\\]\nShow also that the optimal policies are \\(h_k(x) = A^{-1/\\gamma}x\\), for \\(k = N-1,\\ldots,0\\).\n\n\nExercise 1.5 Consider now the infinite-horizon version of the above compsumption problem.\n\nWrite down the associated Bellman equation.\nArgue why a solution to the Bellman equation should be the form\n\n\\[\nv(x) = cx^{1-\\gamma},\n\\]\nwhere \\(c\\) is constant. Find the constant \\(c\\) and the stationary optimal policy.\n\n\nExercise 1.6 Let \\(\\{\\xi_k\\}\\) be a dynamics of iid random variables such that \\(E\\left[\\xi\\right] = 0\\) and \\(E\\left[\\xi^2\\right] = d\\). Consider the dynamics \\[\nx_{k+1} = x_k + a_k + \\xi_k, k = 0,1,2,\\ldots,\n\\]\nand the discounted cost \\[\nE\\left[ \\sum \\beta^k \\left(a^2_k + x_k^2\\right)\\right]\n\\]\n\nWrite down the associated Bellman equation.\nConjecture that the solution to the Bellman equation takes the form \\(v(x) = ax^2 + b\\), where \\(a\\) and \\(b\\) are constant.\nDetermine the constants \\(a\\) and \\(b\\).\nConjecture that the solution to the Bellman equation takes the form \\(v(x) = ax^2 + b\\), where \\(a\\) and \\(b\\) are constant. Determine the constants \\(a\\) and \\(b\\)."
  }
]